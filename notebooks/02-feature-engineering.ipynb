{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765f7de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Loaded processed summary: ../data/processed/phish_url_summary.csv with shape (53657, 2)\n",
      "Loaded processed summary: ../data/processed/enron_senders.csv with shape (1000, 14)\n",
      "Loaded processed summary: ../data/processed/phish_url_summary.csv with shape (53657, 2)\n",
      "Loaded processed summary: ../data/processed/enron_senders.csv with shape (1000, 14)\n",
      "Saved features to ../data/processed/phishing_features.csv with shape (53657, 15)\n",
      "Saved features to ../data/processed/phishing_features.csv with shape (53657, 15)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'body'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/phishing-risk-detection-project/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'body'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 210\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps?://[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(text))\n\u001b[0;32m--> 210\u001b[0m enron_senders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murls\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43menron_senders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(extract_urls)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# 3. Expand rows for each URL\u001b[39;00m\n\u001b[1;32m    213\u001b[0m enron_expanded \u001b[38;5;241m=\u001b[39m enron_senders\u001b[38;5;241m.\u001b[39mexplode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murls\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/phishing-risk-detection-project/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/phishing-risk-detection-project/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'body'"
     ]
    }
   ],
   "source": [
    "# 02-feature-engineering.ipynb\n",
    "# Feature Engineering: URL & Metadata\n",
    "\n",
    "# Ensure required packages are available in the notebook environment\n",
    "# Use the notebook magic to install missing packages if needed.\n",
    "# This is necessary to avoid ModuleNotFoundError in interactive sessions.\n",
    "%pip install --quiet pandas numpy tldextract python-whois\n",
    "\n",
    "\"\"\"\n",
    "## 02 - Feature Engineering\n",
    "\n",
    "This notebook develops core features for phishing detection:\n",
    "\n",
    "1. Load summary CSVs from Notebook 01 (phish_url_summary, enron_senders).\n",
    "2. Extract URLs from email bodies and build unified DataFrame.\n",
    "3. Compute lexical URL features (length, special chars, token counts).\n",
    "4. Parse domain information using tldextract and WHOIS (domain age).\n",
    "5. Identify IP-based URLs and subdomain counts.\n",
    "6. Merge sender-domain mismatch flags.\n",
    "7. Save engineered features to CSV for model input.\n",
    "\"\"\"\n",
    "\n",
    "#%%\n",
    "# 1. Imports and Config\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tldextract\n",
    "import whois\n",
    "from datetime import datetime\n",
    "\n",
    "# Notebook options\n",
    "USE_WHOIS = False  # set True to enable network WHOIS lookups (may be slow)\n",
    "WHOIS_CACHE = os.path.join('..','data','processed','whois_cache.csv')\n",
    "\n",
    "# Reduce verbosity from whois library\n",
    "logging.getLogger('whois.whois').setLevel(logging.ERROR)\n",
    "\n",
    "# Simple cache helpers\n",
    "def load_whois_cache(path):\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            return pd.read_csv(path, index_col=0)\n",
    "        except Exception:\n",
    "            return pd.DataFrame(columns=['domain','age_days']).set_index('domain')\n",
    "    return pd.DataFrame(columns=['domain','age_days']).set_index('domain')\n",
    "\n",
    "def save_whois_cache(df, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    df.to_csv(path)\n",
    "\n",
    "whois_cache = load_whois_cache(WHOIS_CACHE)\n",
    "\n",
    "# File paths\n",
    "PHISH_SUMMARY = os.path.join('..','data','processed','phish_url_summary.csv')\n",
    "ENRON_SENDERS = os.path.join('..','data','processed','enron_senders.csv')\n",
    "OUTPUT_FEATURES = os.path.join('..','data','processed','phishing_features.csv')\n",
    "\n",
    "#%%\n",
    "# 2. Load Summaries (robust)\n",
    "def load_summary(processed_path, raw_path, key_col='url'):\n",
    "    \"\"\"Try to load a processed summary CSV, fall back to a raw CSV if present.\"\"\"\n",
    "    import os\n",
    "    if os.path.exists(processed_path):\n",
    "        try:\n",
    "            df = pd.read_csv(processed_path)\n",
    "            print(f'Loaded processed summary: {processed_path} with shape {df.shape}')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f'Failed to read processed file {processed_path}: {e}')\n",
    "    # processed missing or unreadable -> try raw\n",
    "    if os.path.exists(raw_path) and os.path.getsize(raw_path) > 0:\n",
    "        try:\n",
    "            df = pd.read_csv(raw_path, names=[key_col], header=0) if os.path.getsize(raw_path) > 0 else pd.DataFrame(columns=[key_col])\n",
    "            print(f'Loaded raw file: {raw_path} with shape {df.shape}')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f'Failed to read raw file {raw_path}: {e}')\n",
    "    # Last resort: return empty dataframe with expected columns\n",
    "    print('No summary files found or files empty â€” creating empty DataFrame with `url` column')\n",
    "    return pd.DataFrame(columns=[key_col])\n",
    "\n",
    "phish_df = load_summary(PHISH_SUMMARY, os.path.join('..','data','raw','phishtank_urls.csv'))\n",
    "enron_senders = load_summary(ENRON_SENDERS, os.path.join('..','data','processed','enron_senders.csv'), key_col='sender')\n",
    "\n",
    "#%%\n",
    "# 3. Expand URLs (if multiple per row) - demonstration on phishing URLs\n",
    "# Ensure expected column exists and normalize\n",
    "if 'url' not in phish_df.columns:\n",
    "    # If the raw phishtank file used a different column name, try to find a URL-like column\n",
    "    url_cols = [c for c in phish_df.columns if 'url' in c.lower() or 'link' in c.lower()]\n",
    "    if url_cols:\n",
    "        phish_df = phish_df.rename(columns={url_cols[0]: 'url'})\n",
    "    else:\n",
    "        phish_df['url'] = None\n",
    "\n",
    "expanded = phish_df.copy()\n",
    "# Coerce URL column to string to avoid apply errors on NaN\n",
    "expanded['url'] = expanded['url'].astype('string')\n",
    "# Assume one URL per row; for emails, use extract_urls on bodies.\n",
    "\n",
    "#%%\n",
    "# 4. Lexical features\n",
    "expanded['num_dots'] = expanded['url'].apply(lambda u: u.count('.') if pd.notna(u) else 0)\n",
    "expanded['num_hyphens'] = expanded['url'].apply(lambda u: u.count('-') if pd.notna(u) else 0)\n",
    "expanded['num_underscores'] = expanded['url'].apply(lambda u: u.count('_') if pd.notna(u) else 0)\n",
    "expanded['num_qm'] = expanded['url'].apply(lambda u: u.count('?') if pd.notna(u) else 0)\n",
    "expanded['has_at'] = expanded['url'].apply(lambda u: 1 if (pd.notna(u) and '@' in u) else 0)\n",
    "expanded['path_length'] = expanded['url'].apply(lambda u: len(re.sub(r\"https?://[\\\\w\\\\.]+\", '', u)) if pd.notna(u) else 0)\n",
    "\n",
    "#%%\n",
    "# 5. Domain parsing and age\n",
    "\n",
    "def get_domain_info(url):\n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    ext = tldextract.extract(str(url))\n",
    "    domain = f\"{ext.domain}.{ext.suffix}\" if ext.suffix else ext.domain\n",
    "    return domain\n",
    "\n",
    "expanded['domain'] = expanded['url'].apply(get_domain_info)\n",
    "\n",
    "# WHOIS-based age with cache and optional network\n",
    "from time import sleep\n",
    "\n",
    "def calc_domain_age(domain):\n",
    "    if domain is None or pd.isna(domain):\n",
    "        return np.nan\n",
    "    # check cache first\n",
    "    if domain in whois_cache.index:\n",
    "        return float(whois_cache.loc[domain,'age_days'])\n",
    "    if not USE_WHOIS:\n",
    "        return np.nan\n",
    "    try:\n",
    "        info = whois.whois(domain)\n",
    "        date = info.creation_date\n",
    "        if isinstance(date, list):\n",
    "            date = date[0]\n",
    "        if date is None:\n",
    "            age = np.nan\n",
    "        else:\n",
    "            age = (datetime.now() - date).days\n",
    "        # update cache\n",
    "        whois_cache.loc[domain] = [age]\n",
    "        # be polite to WHOIS servers\n",
    "        sleep(0.5)\n",
    "        return age\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# compute domain_age_days\n",
    "expanded['domain_age_days'] = expanded['domain'].apply(calc_domain_age)\n",
    "\n",
    "# persist cache\n",
    "save_whois_cache(whois_cache, WHOIS_CACHE)\n",
    "\n",
    "#%%\n",
    "# 6. IP-based URL and subdomain count\n",
    "import ipaddress\n",
    "\n",
    "def has_ip(url):\n",
    "    if pd.isna(url):\n",
    "        return 0\n",
    "    try:\n",
    "        host = re.findall(r\"https?://([^/]+)/?\", str(url))[0]\n",
    "        # strip possible port\n",
    "        host = host.split(':')[0]\n",
    "        ipaddress.ip_address(host)\n",
    "        return 1\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "expanded['has_ip'] = expanded['url'].apply(has_ip)\n",
    "expanded['subdomain_count'] = expanded['url'].apply(lambda u: u.split('://')[-1].count('.') - 1 if pd.notna(u) else 0)\n",
    "\n",
    "#%%\n",
    "# 7. Sender-domain mismatch flag\n",
    "# Load email-level data if needed; here we simulate for phishing: assume sender domain known\n",
    "# For phishing URLs, label sender_domain as NaN; feature = 0\n",
    "expanded['sender_domain'] = np.nan\n",
    "expanded['sender_domain_mismatch'] = 0  # to be computed when email data available\n",
    "\n",
    "#%%\n",
    "# 8. Label and save\n",
    "expanded['label'] = 1  # phishing\n",
    "# Ensure output directory exists\n",
    "out_dir = os.path.dirname(OUTPUT_FEATURES)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "expanded.to_csv(OUTPUT_FEATURES, index=False)\n",
    "print(f\"Saved features to {OUTPUT_FEATURES} with shape {expanded.shape}\")\n",
    "\n",
    "#%% [markdown]\n",
    "# --- Benign (Enron) Feature Engineering ---\n",
    "\n",
    "#%%\n",
    "\n",
    "# 1. Load Enron senders/emails (already loaded as enron_senders)\n",
    "# If not loaded, uncomment:\n",
    "# ENRON_SENDERS = os.path.join('..','data','processed','enron_senders.csv')\n",
    "# enron_senders = pd.read_csv(ENRON_SENDERS)\n",
    "\n",
    "# 2. Extract URLs from email bodies\n",
    "def extract_urls(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    return re.findall(r'https?://[^\\s]+', str(text))\n",
    "\n",
    "enron_senders['urls'] = enron_senders['body'].apply(extract_urls)\n",
    "\n",
    "# 3. Expand rows for each URL\n",
    "enron_expanded = enron_senders.explode('urls').reset_index(drop=True)\n",
    "enron_expanded = enron_expanded[enron_expanded['urls'].notna()]\n",
    "enron_expanded = enron_expanded.rename(columns={'urls': 'url'})\n",
    "\n",
    "# 4. Feature engineering (match phishing features)\n",
    "enron_expanded['num_dots'] = enron_expanded['url'].apply(lambda u: u.count('.') if pd.notna(u) else 0)\n",
    "enron_expanded['num_hyphens'] = enron_expanded['url'].apply(lambda u: u.count('-') if pd.notna(u) else 0)\n",
    "enron_expanded['num_underscores'] = enron_expanded['url'].apply(lambda u: u.count('_') if pd.notna(u) else 0)\n",
    "enron_expanded['num_qm'] = enron_expanded['url'].apply(lambda u: u.count('?') if pd.notna(u) else 0)\n",
    "enron_expanded['has_at'] = enron_expanded['url'].apply(lambda u: 1 if (pd.notna(u) and '@' in u) else 0)\n",
    "enron_expanded['path_length'] = enron_expanded['url'].apply(lambda u: len(re.sub(r\"https?://[\\w\\.]+\", '', u)) if pd.notna(u) else 0)\n",
    "\n",
    "def get_domain_info(url):\n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    ext = tldextract.extract(str(url))\n",
    "    domain = f\"{ext.domain}.{ext.suffix}\" if ext.suffix else ext.domain\n",
    "    return domain\n",
    "\n",
    "enron_expanded['domain'] = enron_expanded['url'].apply(get_domain_info)\n",
    "\n",
    "def has_ip(url):\n",
    "    if pd.isna(url):\n",
    "        return 0\n",
    "    try:\n",
    "        host = re.findall(r\"https?://([^/]+)/?\", str(url))[0]\n",
    "        host = host.split(':')[0]\n",
    "        ipaddress.ip_address(host)\n",
    "        return 1\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "enron_expanded['has_ip'] = enron_expanded['url'].apply(has_ip)\n",
    "enron_expanded['subdomain_count'] = enron_expanded['url'].apply(lambda u: u.split('://')[-1].count('.') - 1 if pd.notna(u) else 0)\n",
    "\n",
    "enron_expanded['sender_domain'] = enron_expanded['sender'].apply(lambda s: s.split('@')[-1] if pd.notna(s) and '@' in s else np.nan)\n",
    "enron_expanded['sender_domain_mismatch'] = enron_expanded.apply(\n",
    "    lambda row: int(row['sender_domain'] != row['domain']) if pd.notna(row['sender_domain']) and pd.notna(row['domain']) else 0,\n",
    "    axis=1\n",
    ")\n",
    "enron_expanded['domain_age_days'] = np.nan  # or use WHOIS if you want\n",
    "\n",
    "enron_expanded['label'] = 0  # benign\n",
    "\n",
    "# 5. Save benign features\n",
    "BENIGN_FEATURES = os.path.join('..','data','processed','benign_features.csv')\n",
    "enron_expanded.to_csv(BENIGN_FEATURES, index=False)\n",
    "print(f\"Saved benign features to {BENIGN_FEATURES} with shape {enron_expanded.shape}\")\n",
    "\n",
    "#%%\n",
    "\n",
    "# 6. Combine phishing and benign features for modeling\n",
    "PHISHING_FEATURES = os.path.join('..','data','processed','phishing_features.csv')\n",
    "benign_features = pd.read_csv(BENIGN_FEATURES)\n",
    "phishing_features = pd.read_csv(PHISHING_FEATURES)\n",
    "all_features = pd.concat([phishing_features, benign_features], ignore_index=True)\n",
    "ALL_FEATURES = os.path.join('..','data','processed','phishing_graph_features.csv')\n",
    "all_features.to_csv(ALL_FEATURES, index=False)\n",
    "print(f\"Combined phishing and benign features saved to {ALL_FEATURES} with shape {all_features.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
