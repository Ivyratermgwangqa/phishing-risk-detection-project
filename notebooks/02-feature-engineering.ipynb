{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765f7de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "No summary files found or files empty — creating empty DataFrame with `url` column\n",
      "No summary files found or files empty — creating empty DataFrame with `url` column\n",
      "No summary files found or files empty — creating empty DataFrame with `url` column\n",
      "No summary files found or files empty — creating empty DataFrame with `url` column\n",
      "Saved features to ../data/processed/phishing_features.csv with shape (0, 14)\n",
      "Saved features to ../data/processed/phishing_features.csv with shape (0, 14)\n"
     ]
    }
   ],
   "source": [
    "# 02-feature-engineering.ipynb\n",
    "# Feature Engineering: URL & Metadata\n",
    "\n",
    "# Ensure required packages are available in the notebook environment\n",
    "# Use the notebook magic to install missing packages if needed.\n",
    "# This is necessary to avoid ModuleNotFoundError in interactive sessions.\n",
    "%pip install --quiet pandas numpy tldextract python-whois\n",
    "\n",
    "\"\"\"\n",
    "## 02 - Feature Engineering\n",
    "\n",
    "This notebook develops core features for phishing detection:\n",
    "\n",
    "1. Load summary CSVs from Notebook 01 (phish_url_summary, enron_senders).\n",
    "2. Extract URLs from email bodies and build unified DataFrame.\n",
    "3. Compute lexical URL features (length, special chars, token counts).\n",
    "4. Parse domain information using tldextract and WHOIS (domain age).\n",
    "5. Identify IP-based URLs and subdomain counts.\n",
    "6. Merge sender-domain mismatch flags.\n",
    "7. Save engineered features to CSV for model input.\n",
    "\"\"\"\n",
    "\n",
    "#%%\n",
    "# 1. Imports and Config\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tldextract\n",
    "import whois\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_urls(text):\n",
    "    \"\"\"\n",
    "    Fallback URL extractor when a local 'utils' module is not available.\n",
    "    Returns a list of http/https URLs found in the input text.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    pattern = r\"https?://[^\\s'\\\"<>]+\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# File paths\n",
    "PHISH_SUMMARY = os.path.join('..','data','processed','phish_url_summary.csv')\n",
    "ENRON_SENDERS = os.path.join('..','data','processed','enron_senders.csv')\n",
    "OUTPUT_FEATURES = os.path.join('..','data','processed','phishing_features.csv')\n",
    "\n",
    "#%%\n",
    "# 2. Load Summaries (robust)\n",
    "def load_summary(processed_path, raw_path, key_col='url'):\n",
    "    \"\"\"Try to load a processed summary CSV, fall back to a raw CSV if present.\"\"\"\n",
    "    import os\n",
    "    if os.path.exists(processed_path):\n",
    "        try:\n",
    "            df = pd.read_csv(processed_path)\n",
    "            print(f'Loaded processed summary: {processed_path} with shape {df.shape}')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f'Failed to read processed file {processed_path}: {e}')\n",
    "    # processed missing or unreadable -> try raw\n",
    "    if os.path.exists(raw_path) and os.path.getsize(raw_path) > 0:\n",
    "        try:\n",
    "            df = pd.read_csv(raw_path, names=[key_col], header=0) if os.path.getsize(raw_path) > 0 else pd.DataFrame(columns=[key_col])\n",
    "            print(f'Loaded raw file: {raw_path} with shape {df.shape}')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f'Failed to read raw file {raw_path}: {e}')\n",
    "    # Last resort: return empty dataframe with expected columns\n",
    "    print('No summary files found or files empty — creating empty DataFrame with `url` column')\n",
    "    return pd.DataFrame(columns=[key_col])\n",
    "\n",
    "phish_df = load_summary(PHISH_SUMMARY, os.path.join('..','data','raw','phishtank_urls.csv'))\n",
    "enron_senders = load_summary(ENRON_SENDERS, os.path.join('..','data','processed','enron_senders.csv'), key_col='sender')\n",
    "\n",
    "#%%\n",
    "# 3. Expand URLs (if multiple per row) - demonstration on phishing URLs\n",
    "# Ensure expected column exists and normalize\n",
    "if 'url' not in phish_df.columns:\n",
    "    # If the raw phishtank file used a different column name, try to find a URL-like column\n",
    "    url_cols = [c for c in phish_df.columns if 'url' in c.lower() or 'link' in c.lower()]\n",
    "    if url_cols:\n",
    "        phish_df = phish_df.rename(columns={url_cols[0]: 'url'})\n",
    "    else:\n",
    "        phish_df['url'] = None\n",
    "\n",
    "expanded = phish_df.copy()\n",
    "# Coerce URL column to string to avoid apply errors on NaN\n",
    "expanded['url'] = expanded['url'].astype('string')\n",
    "# Assume one URL per row; for emails, use extract_urls on bodies.\n",
    "\n",
    "#%%\n",
    "# 4. Lexical features\n",
    "expanded['num_dots'] = expanded['url'].apply(lambda u: u.count('.') if pd.notna(u) else 0)\n",
    "expanded['num_hyphens'] = expanded['url'].apply(lambda u: u.count('-') if pd.notna(u) else 0)\n",
    "expanded['num_underscores'] = expanded['url'].apply(lambda u: u.count('_') if pd.notna(u) else 0)\n",
    "expanded['num_qm'] = expanded['url'].apply(lambda u: u.count('?') if pd.notna(u) else 0)\n",
    "expanded['has_at'] = expanded['url'].apply(lambda u: 1 if (pd.notna(u) and '@' in u) else 0)\n",
    "expanded['path_length'] = expanded['url'].apply(lambda u: len(re.sub(r\"https?://[\\\\w\\\\.]+\", '', u)) if pd.notna(u) else 0)\n",
    "\n",
    "#%%\n",
    "# 5. Domain parsing and age\n",
    "\n",
    "def get_domain_info(url):\n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    ext = tldextract.extract(str(url))\n",
    "    domain = f\"{ext.domain}.{ext.suffix}\" if ext.suffix else ext.domain\n",
    "    return domain\n",
    "\n",
    "expanded['domain'] = expanded['url'].apply(get_domain_info)\n",
    "\n",
    "# WHOIS-based age\n",
    "def calc_domain_age(domain):\n",
    "    if domain is None or pd.isna(domain):\n",
    "        return np.nan\n",
    "    try:\n",
    "        info = whois.whois(domain)\n",
    "        date = info.creation_date\n",
    "        if isinstance(date, list): date = date[0]\n",
    "        if date is None:\n",
    "            return np.nan\n",
    "        return (datetime.now() - date).days\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "expanded['domain_age_days'] = expanded['domain'].apply(calc_domain_age)\n",
    "\n",
    "#%%\n",
    "# 6. IP-based URL and subdomain count\n",
    "import ipaddress\n",
    "\n",
    "def has_ip(url):\n",
    "    if pd.isna(url):\n",
    "        return 0\n",
    "    try:\n",
    "        host = re.findall(r\"https?://([^/]+)/?\", str(url))[0]\n",
    "        # strip possible port\n",
    "        host = host.split(':')[0]\n",
    "        ipaddress.ip_address(host)\n",
    "        return 1\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "expanded['has_ip'] = expanded['url'].apply(has_ip)\n",
    "expanded['subdomain_count'] = expanded['url'].apply(lambda u: u.split('://')[-1].count('.') - 1 if pd.notna(u) else 0)\n",
    "\n",
    "#%%\n",
    "# 7. Sender-domain mismatch flag\n",
    "# Load email-level data if needed; here we simulate for phishing: assume sender domain known\n",
    "# For phishing URLs, label sender_domain as NaN; feature = 0\n",
    "expanded['sender_domain'] = np.nan\n",
    "expanded['sender_domain_mismatch'] = 0  # to be computed when email data available\n",
    "\n",
    "#%%\n",
    "# 8. Label and save\n",
    "expanded['label'] = 1  # phishing\n",
    "# Ensure output directory exists\n",
    "out_dir = os.path.dirname(OUTPUT_FEATURES)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "expanded.to_csv(OUTPUT_FEATURES, index=False)\n",
    "print(f\"Saved features to {OUTPUT_FEATURES} with shape {expanded.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
